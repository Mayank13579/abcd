{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d4b1895-5a72-44a8-b288-a899d157bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ed8f132-5893-44e3-bc80-bd927cb745a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading WordNet: Package 'WordNet' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('WordNet')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f218dd78-0531-4470-afe2-f2a2e655f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"my name is hemant Kumar singh. i am also know as Bagula, Huehuehueheuhe. i am from chhapra yet i'm not a chhapri, huehueheuh.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66eac6b1-c242-4479-a880-cd1e00344e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is hemant Kumar singh. i am also know as Bagula, Huehuehueheuhe. i am from chhapra yet i'm not a chhapri, huehueheuh.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80796b9d-ece3-4282-a653-6f2c87934a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'hemant', 'Kumar', 'singh', '.', 'i', 'am', 'also', 'know', 'as', 'Bagula', ',', 'Huehuehueheuhe', '.', 'i', 'am', 'from', 'chhapra', 'yet', 'i', \"'m\", 'not', 'a', 'chhapri', ',', 'huehueheuh', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ec1277-8079-4784-9cb9-a31562703e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my name is hemant Kumar singh.', 'i am also know as Bagula, Huehuehueheuhe.', \"i am from chhapra yet i'm not a chhapri, huehueheuh.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1af8da77-42d8-4e84-b15a-d67ddca4efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pos = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b57f377-61cb-46e1-a120-bfe76791c681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('hemant', 'JJ'), ('Kumar', 'NNP'), ('singh', 'NN'), ('.', '.'), ('i', 'NN'), ('am', 'VBP'), ('also', 'RB'), ('know', 'VBP'), ('as', 'IN'), ('Bagula', 'NNP'), (',', ','), ('Huehuehueheuhe', 'NNP'), ('.', '.'), ('i', 'NN'), ('am', 'VBP'), ('from', 'IN'), ('chhapra', 'NN'), ('yet', 'RB'), ('i', 'JJ'), (\"'m\", 'VBP'), ('not', 'RB'), ('a', 'DT'), ('chhapri', 'NN'), (',', ','), ('huehueheuh', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(to_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af12e5bc-e1ea-4a9b-a16c-20aea84ae0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2968b52-2698-4f15-8098-8acaaf5ee5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'each', 'be', 'her', \"it's\", 'its', 'ourselves', 'it', 'the', 'by', 'a', 'here', 'are', 'in', 'can', 'which', \"you've\", 'y', \"hasn't\", 'again', 'd', 'further', 'more', 'he', 'ain', 'i', 'wasn', 'through', 'or', 'is', \"mustn't\", 'she', 'there', 'now', \"wasn't\", 't', \"you'll\", 'up', 'why', 've', 'my', 'where', 'your', 'had', 'mustn', 'nor', 'me', \"you'd\", 'themselves', 'shouldn', 'down', 'few', 'before', 'because', 'being', 'once', 'those', 'doing', 'we', 'aren', 'his', \"aren't\", 'ma', \"shouldn't\", 'as', 'didn', 'our', 'won', 'no', \"couldn't\", 'them', 'on', 'hers', 'him', 'against', 'll', 'over', 'out', 'such', 'most', 'than', 'm', 'between', 'their', 'both', 'shan', 'himself', 'am', 'has', \"won't\", \"wouldn't\", 'not', 'wouldn', \"haven't\", 'you', 'myself', \"don't\", 'for', 'was', 'itself', \"should've\", \"hadn't\", 'into', 'with', 'don', 're', 'yourselves', 'only', 'will', 'were', 'some', 'yourself', 'to', 'while', 'if', 'that', 'about', 'above', \"she's\", 'weren', 'all', 'then', 'just', 'couldn', 'what', 'yours', 'isn', \"needn't\", 'o', 'under', 'of', 'until', 'do', 'own', \"you're\", \"mightn't\", 'same', 'but', 'ours', 'too', 'hadn', 'these', 'doesn', 'should', 'an', 'during', 'how', 'who', 'hasn', 'very', \"doesn't\", 'from', 'needn', 'did', 'haven', 'they', 'theirs', \"that'll\", 'does', 'mightn', 'so', 'having', 'after', 'below', \"didn't\", 'this', 'off', \"isn't\", 'been', 'have', \"shan't\", 'whom', 'at', 's', 'herself', \"weren't\", 'when', 'other', 'and', 'any'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1e84b3d-6bf3-4b10-8ae2-b88b42a59eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f59cd55-f4dd-4ff8-bf74-65f3a9648ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'hemant', 'Kumar', 'singh', '.', 'also', 'know', 'Bagula', ',', 'Huehuehueheuhe', '.', 'chhapra', 'yet', \"'m\", 'chhapri', ',', 'huehueheuh', '.']\n"
     ]
    }
   ],
   "source": [
    "no_stopwords = []\n",
    "for token in to_clean:\n",
    "    if token not in stop_words:\n",
    "        no_stopwords.append(token)\n",
    "print(no_stopwords)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d351952e-a68d-47ef-9b8e-fa31a0187b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58965f3b-2942-4f09-87fd-1c67ad0b3e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'hemant', 'kumar', 'singh', '.', 'also', 'know', 'bagula', ',', 'huehuehueheuh', '.', 'chhapra', 'yet', \"'m\", 'chhapri', ',', 'huehueheuh', '.']\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for token in no_stopwords:\n",
    "    y = stemmer.stem(token)\n",
    "    x.append(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdc0ce48-40a5-4587-84ce-3f01071a3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83501588-43b2-4cd6-bf2d-efc13c7c3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'hemant', 'Kumar', 'singh', '.', 'also', 'know', 'Bagula', ',', 'Huehuehueheuhe', '.', 'chhapra', 'yet', \"'m\", 'chhapri', ',', 'huehueheuh', '.']\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "for token in no_stopwords:\n",
    "    b = lemmatizer.lemmatize(token)\n",
    "    a.append(b)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a04c5-0482-407d-8e6e-757bb470e2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
